Subreddit Scraper Instruction Manual
------------------------------------
Welcome to the Subreddit Scraper Tool! Whether this is the first time you tried web scraping before or you acquired some prior knowledge in it, this documentation will guide you on how to utilize Web Scrapy written in Python language in order to retrieve subreddit data from Reddit.com. For more information, be sure to check out the documentation below:
https://docs.scrapy.org/en/latest/
(NOTE: In order to keep this wiki guide simple and tidy as possible, I will try to summarize only the crucial key points in the Scrapy documentation.)

Important Things to Remember:
------------------------------------
name="subreddits"
Notice that name up there? That's what gives the Spider program an identity name, in which we will be using when entering the command "scrapy crawl <name>". In other words, whenever you give a Spider a name, be sure to remember it when you want to run the program. Did I forget to mention that you can't reuse the name once you assigned it to the Spider?

start_requests()
You can't go wrong about this function you declared. Without it, you won't be able to do any of the web scraping. start_requests() will assign the Spider a task to crawl through the given URL in order to return an iterable list of Requests. You can expect that further requests will be generated as the initial ones have been processed.

parse()
The parse() function will be returning all the downloaded responses acquired by requests. In other words, a complete list of the scraped data will be sorted into dictionaries and search for new URLs and create additional requests when needed.

response.css('lorem ipsum')
If you are wondering what 'css' stands for, it is the cascading style sheets, in which they
provide the layout of the web pages. So when the response.css() is called, the scraper will return a list Selector objects wrapping around the HTML elements and permit the extraction of the necessary data.

scraped_info = {'insert name here' : item[n],}
In order to store the scraped info the Scraper has collected, you may consider to define a dictionary consisting of the title and whatever data you want to put in. In other words, treat item[n] as an array of scraped data stored and be sure to include a comma in every line of title name and the item[n] array.

Closing Note:
------------------------------------
By the time you finished reading this helpful guide on the Subreddit Scraper program powered by Python, at least you will be familiar on how to retrive internet data from the world wide web and place it into any file for later reference. That way, in case that you want to check the contents of the crawled web data for your own use, you can count on the documentation to print out whatever information you wish to read from.

In other words, Happy Scrapping!
